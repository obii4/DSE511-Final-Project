{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f932aae2",
   "metadata": {},
   "source": [
    "Christopher O'Brien\n",
    "\n",
    "DSE 511 - Project 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a779feb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import preprocessor as p\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5ef129",
   "metadata": {},
   "source": [
    "# DATASET DESCRIPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cadd554",
   "metadata": {},
   "source": [
    "The dataset analyzed was obtained from Kaggle’s ‘Natural Language Processing with Disaster Tweets’ competition. From the competition, we were instructed to use only the \"train.csv\" file. The contained a dataset of 7613 tweets. Each row of the dataset contained the following information: id, keyword, location, text, and target. The task of the project was to classify whether each tweet was discussing a real disaster or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8c2f0e",
   "metadata": {},
   "source": [
    "# FEATURE EXTRACTION  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f67373",
   "metadata": {},
   "source": [
    "To begin, I started by preprocessing the dataset. This was first achieved through pip installing tweet-preprocessor. I cleaned the text data by removing the following items: URLs, Hashtags, Mentions, Reserved words (RT, FAV), Emojis, Smileys. My final step of preprocessing was that I removed blank spaces and punctuation. I next processed the preprocessed text into ML compatible features using TfidfVectorizer. I chose to use the TfidfVectorizer instead of CountVectorizer to hopefully not introduce a bias for the most frequent words that appear in the dataset. The main parameters that I tinkered with were min_df and ngram_range. It was determined that a min_df=3 was ideal. Additionally, I found the use of unigrams and bigrams performed the best with my initial testing. When only unigrams or bigrams were used the performance suffered. I also found that performance was improved if the tweets were converted to lowercase through the TfidfVectorizer. I used a 70/15/15 split to separate out the vectorized data into train, validation, and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b2fd2d",
   "metadata": {},
   "source": [
    "## Data Import and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "887acc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/88/6qzh7bb57y7g_z7dxyz4hsy00000gn/T/ipykernel_35331/497194790.py:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  tweets['text'] = tweets['text'].str.replace('[^\\w\\s]',' ').str.replace('\\s\\s+', ' ')\n"
     ]
    }
   ],
   "source": [
    "tweets = pd.read_csv(\"train.csv\") #load all data\n",
    "\n",
    "#used a tweet processer from here - https://pypi.org/project/tweet-preprocessor/\n",
    "#it removed URLs, Hashtags, Mentions, Reserved words (RT, FAV), Emojis, Smileys\n",
    "def preprocess_tweet(row):\n",
    "    text = row['text']\n",
    "    text = p.clean(text)\n",
    "    return text\n",
    "\n",
    "#apply preprocess function\n",
    "tweets['text'] = tweets.apply(preprocess_tweet, axis=1)\n",
    "\n",
    "#remove blank spaces and punctuation\n",
    "tweets['text'] = tweets['text'].str.replace('[^\\w\\s]',' ').str.replace('\\s\\s+', ' ')\n",
    "\n",
    "dC_x_fin = tweets['text'] #only tweets\n",
    "dC_y = tweets['target'] #only labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2280328d",
   "metadata": {},
   "source": [
    "### Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3c502cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# process raw text into ML compatible features\n",
    "vectorizer = TfidfVectorizer(min_df=3, \n",
    "             stop_words='english',ngram_range=(1, 2), lowercase=True)  \n",
    "vectorizer.fit(dC_x_fin)\n",
    "#print(vectorizer.vocabulary_)\n",
    "X = vectorizer.transform(dC_x_fin)\n",
    "#vectorizer.get_stop_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae073d",
   "metadata": {},
   "source": [
    "#### Tfidf Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67aa9b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, dC_y, \n",
    "                                   test_size=0.15, shuffle=True, stratify=dC_y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                 test_size=0.15/0.85, shuffle=True, stratify=y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "513352e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: (7613, 5826) (7613,)\n",
      "train: (5329, 5826) (5329,)\n",
      "val: (1142, 5826) (1142,)\n",
      "test: (1142, 5826) (1142,)\n"
     ]
    }
   ],
   "source": [
    "print('original:',X.shape,dC_y.shape)\n",
    "print('train:',X_train.shape,y_train.shape)\n",
    "print('val:',X_val.shape,y_val.shape)\n",
    "print('test:',X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0485350",
   "metadata": {},
   "source": [
    "# MODELS AND HYPERPARAMETER OPTIMIZATION "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6099db",
   "metadata": {},
   "source": [
    "The machine learning models that I chose to implement were: Logistic Regression, RandomForestClassifier, and LinearSVC. For hyperparameter optimization, I decided to use GridSearchCV. Logistic Regression was my starting point for the analyses of the text data. For my exhaustive grid search I examined the following hyperparameters: solvers, penalty, and C. To begin, I chose an arbitrary range of C values. Once a semi-optimal value was obtained, I attempted to narrow the C value in some. Next, I used a RandomForestClassifier. I chose this model mainly because I have no prior experience working with it at all. This model took by far the longest time do exhaustive grid search on, largely due to the models run time and the number of hyperparameters evaluated. The hyperparameters that I examined were: max_features , max_depth, min_samples_split, min_samples_leaf, and bootstrap. Finally, I implemented the LinearSVC model. This specific model was chosen after examining the ‘sci-kit learn algorithm cheat sheet’ that was presented at the beginning of our lecture. The hyperparameters that I evaluated were penalty, loss, and C values. Just like with Logistic Regression I attempted to find an optimal C value the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2928d57",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f953db0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.739987 using {'C': 1.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.739987 (0.030535) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.739987 (0.030535) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.739110 (0.030041) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.739110 (0.030041) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'saga'}\n",
      "0.739987 (0.030535) with: {'C': 1.0, 'penalty': 'l2', 'solver': 'sag'}\n",
      "0.739110 (0.030041) with: {'C': 1.02222222, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.739110 (0.030041) with: {'C': 1.02222222, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.739106 (0.029039) with: {'C': 1.02222222, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.738233 (0.029644) with: {'C': 1.02222222, 'penalty': 'l2', 'solver': 'saga'}\n",
      "0.739110 (0.030041) with: {'C': 1.02222222, 'penalty': 'l2', 'solver': 'sag'}\n",
      "0.738233 (0.030280) with: {'C': 1.04444444, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.738233 (0.030280) with: {'C': 1.04444444, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.738233 (0.030280) with: {'C': 1.04444444, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.738233 (0.030280) with: {'C': 1.04444444, 'penalty': 'l2', 'solver': 'saga'}\n",
      "0.738233 (0.030280) with: {'C': 1.04444444, 'penalty': 'l2', 'solver': 'sag'}\n",
      "0.738229 (0.029806) with: {'C': 1.06666667, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.738229 (0.029806) with: {'C': 1.06666667, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.739102 (0.029464) with: {'C': 1.06666667, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.738225 (0.029190) with: {'C': 1.06666667, 'penalty': 'l2', 'solver': 'saga'}\n",
      "0.738229 (0.029806) with: {'C': 1.06666667, 'penalty': 'l2', 'solver': 'sag'}\n",
      "0.739098 (0.028974) with: {'C': 1.08888889, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.739098 (0.028974) with: {'C': 1.08888889, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.739975 (0.030385) with: {'C': 1.08888889, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.739098 (0.028974) with: {'C': 1.08888889, 'penalty': 'l2', 'solver': 'saga'}\n",
      "0.739098 (0.028974) with: {'C': 1.08888889, 'penalty': 'l2', 'solver': 'sag'}\n",
      "0.739106 (0.033341) with: {'C': 1.11111111, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.739106 (0.033341) with: {'C': 1.11111111, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.739110 (0.034879) with: {'C': 1.11111111, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.739110 (0.034879) with: {'C': 1.11111111, 'penalty': 'l2', 'solver': 'saga'}\n",
      "0.739106 (0.033341) with: {'C': 1.11111111, 'penalty': 'l2', 'solver': 'sag'}\n",
      "0.739110 (0.034658) with: {'C': 1.13333333, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.739110 (0.034658) with: {'C': 1.13333333, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.738233 (0.034537) with: {'C': 1.13333333, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.738233 (0.034537) with: {'C': 1.13333333, 'penalty': 'l2', 'solver': 'saga'}\n",
      "0.739110 (0.034658) with: {'C': 1.13333333, 'penalty': 'l2', 'solver': 'sag'}\n",
      "0.738233 (0.034537) with: {'C': 1.15555556, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.738233 (0.034537) with: {'C': 1.15555556, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.738233 (0.034537) with: {'C': 1.15555556, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.738233 (0.034537) with: {'C': 1.15555556, 'penalty': 'l2', 'solver': 'saga'}\n",
      "0.738233 (0.034537) with: {'C': 1.15555556, 'penalty': 'l2', 'solver': 'sag'}\n",
      "0.739979 (0.033012) with: {'C': 1.17777778, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.739106 (0.034354) with: {'C': 1.17777778, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.739979 (0.033012) with: {'C': 1.17777778, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.739979 (0.033012) with: {'C': 1.17777778, 'penalty': 'l2', 'solver': 'saga'}\n",
      "0.739106 (0.034354) with: {'C': 1.17777778, 'penalty': 'l2', 'solver': 'sag'}\n",
      "0.739102 (0.033025) with: {'C': 1.2, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "0.739102 (0.033025) with: {'C': 1.2, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "0.739975 (0.031747) with: {'C': 1.2, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.739975 (0.031747) with: {'C': 1.2, 'penalty': 'l2', 'solver': 'saga'}\n",
      "0.739102 (0.033025) with: {'C': 1.2, 'penalty': 'l2', 'solver': 'sag'}\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=0, max_iter=1000)\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear', 'saga', 'sag']\n",
    "penalty = ['l2']\n",
    "c_values = [1.        , 1.02222222, 1.04444444, 1.06666667, 1.08888889,\n",
    "       1.11111111, 1.13333333, 1.15555556, 1.17777778, 1.2]\n",
    "\n",
    "#define grid search\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X_val,y_val)\n",
    "\n",
    "#summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d211f52f",
   "metadata": {},
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3cfda8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.747870 using {'bootstrap': True, 'max_depth': 110, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.636624 (0.013252) with: {'bootstrap': True, 'max_depth': 10, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.635750 (0.015438) with: {'bootstrap': True, 'max_depth': 10, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.638374 (0.011936) with: {'bootstrap': True, 'max_depth': 10, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.638374 (0.011936) with: {'bootstrap': True, 'max_depth': 10, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.636624 (0.013252) with: {'bootstrap': True, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.635750 (0.015438) with: {'bootstrap': True, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.638374 (0.011936) with: {'bootstrap': True, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.638374 (0.011936) with: {'bootstrap': True, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.704938 (0.017704) with: {'bootstrap': True, 'max_depth': 35, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.704060 (0.017503) with: {'bootstrap': True, 'max_depth': 35, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.705819 (0.021125) with: {'bootstrap': True, 'max_depth': 35, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.705819 (0.021125) with: {'bootstrap': True, 'max_depth': 35, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.704938 (0.017704) with: {'bootstrap': True, 'max_depth': 35, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.704060 (0.017503) with: {'bootstrap': True, 'max_depth': 35, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.705819 (0.021125) with: {'bootstrap': True, 'max_depth': 35, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.705819 (0.021125) with: {'bootstrap': True, 'max_depth': 35, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.718950 (0.018921) with: {'bootstrap': True, 'max_depth': 60, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.719835 (0.022588) with: {'bootstrap': True, 'max_depth': 60, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.732985 (0.028320) with: {'bootstrap': True, 'max_depth': 60, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.733858 (0.027298) with: {'bootstrap': True, 'max_depth': 60, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.718950 (0.018921) with: {'bootstrap': True, 'max_depth': 60, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.719835 (0.022588) with: {'bootstrap': True, 'max_depth': 60, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.732985 (0.028320) with: {'bootstrap': True, 'max_depth': 60, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.733858 (0.027298) with: {'bootstrap': True, 'max_depth': 60, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.731227 (0.026513) with: {'bootstrap': True, 'max_depth': 85, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.732981 (0.027251) with: {'bootstrap': True, 'max_depth': 85, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.736478 (0.024883) with: {'bootstrap': True, 'max_depth': 85, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.737355 (0.026014) with: {'bootstrap': True, 'max_depth': 85, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.731227 (0.026513) with: {'bootstrap': True, 'max_depth': 85, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.732981 (0.027251) with: {'bootstrap': True, 'max_depth': 85, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.736478 (0.024883) with: {'bootstrap': True, 'max_depth': 85, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.737355 (0.026014) with: {'bootstrap': True, 'max_depth': 85, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.732100 (0.026948) with: {'bootstrap': True, 'max_depth': 110, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.732973 (0.024525) with: {'bootstrap': True, 'max_depth': 110, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.744361 (0.025662) with: {'bootstrap': True, 'max_depth': 110, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.747870 (0.030529) with: {'bootstrap': True, 'max_depth': 110, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.732100 (0.026948) with: {'bootstrap': True, 'max_depth': 110, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.732973 (0.024525) with: {'bootstrap': True, 'max_depth': 110, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.744361 (0.025662) with: {'bootstrap': True, 'max_depth': 110, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.747870 (0.030529) with: {'bootstrap': True, 'max_depth': 110, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.699636 (0.023304) with: {'bootstrap': True, 'max_depth': None, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.745242 (0.031008) with: {'bootstrap': True, 'max_depth': None, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.742603 (0.023849) with: {'bootstrap': True, 'max_depth': None, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.746989 (0.028064) with: {'bootstrap': True, 'max_depth': None, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.699636 (0.023304) with: {'bootstrap': True, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.745242 (0.031008) with: {'bootstrap': True, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.742603 (0.023849) with: {'bootstrap': True, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.746989 (0.028064) with: {'bootstrap': True, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.654141 (0.014280) with: {'bootstrap': False, 'max_depth': 10, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.651517 (0.014891) with: {'bootstrap': False, 'max_depth': 10, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.651513 (0.014714) with: {'bootstrap': False, 'max_depth': 10, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.651513 (0.014714) with: {'bootstrap': False, 'max_depth': 10, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.654141 (0.014280) with: {'bootstrap': False, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.651517 (0.014891) with: {'bootstrap': False, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.651513 (0.014714) with: {'bootstrap': False, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.651513 (0.014714) with: {'bootstrap': False, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.716322 (0.017234) with: {'bootstrap': False, 'max_depth': 35, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.715452 (0.020507) with: {'bootstrap': False, 'max_depth': 35, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.714579 (0.021280) with: {'bootstrap': False, 'max_depth': 35, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.714579 (0.021280) with: {'bootstrap': False, 'max_depth': 35, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.716322 (0.017234) with: {'bootstrap': False, 'max_depth': 35, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.715452 (0.020507) with: {'bootstrap': False, 'max_depth': 35, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.714579 (0.021280) with: {'bootstrap': False, 'max_depth': 35, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.714579 (0.021280) with: {'bootstrap': False, 'max_depth': 35, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.727710 (0.020256) with: {'bootstrap': False, 'max_depth': 60, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.726837 (0.020444) with: {'bootstrap': False, 'max_depth': 60, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.731204 (0.014757) with: {'bootstrap': False, 'max_depth': 60, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.731204 (0.014757) with: {'bootstrap': False, 'max_depth': 60, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.727710 (0.020256) with: {'bootstrap': False, 'max_depth': 60, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.726837 (0.020444) with: {'bootstrap': False, 'max_depth': 60, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.731204 (0.014757) with: {'bootstrap': False, 'max_depth': 60, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.731204 (0.014757) with: {'bootstrap': False, 'max_depth': 60, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.730330 (0.018197) with: {'bootstrap': False, 'max_depth': 85, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.738213 (0.020153) with: {'bootstrap': False, 'max_depth': 85, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.732954 (0.017478) with: {'bootstrap': False, 'max_depth': 85, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.732954 (0.017478) with: {'bootstrap': False, 'max_depth': 85, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.730330 (0.018197) with: {'bootstrap': False, 'max_depth': 85, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.738213 (0.020153) with: {'bootstrap': False, 'max_depth': 85, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.732954 (0.017478) with: {'bootstrap': False, 'max_depth': 85, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.732954 (0.017478) with: {'bootstrap': False, 'max_depth': 85, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.734708 (0.022694) with: {'bootstrap': False, 'max_depth': 110, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.735586 (0.020553) with: {'bootstrap': False, 'max_depth': 110, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.732962 (0.019044) with: {'bootstrap': False, 'max_depth': 110, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.732966 (0.020721) with: {'bootstrap': False, 'max_depth': 110, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.734708 (0.022694) with: {'bootstrap': False, 'max_depth': 110, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.735586 (0.020553) with: {'bootstrap': False, 'max_depth': 110, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.732962 (0.019044) with: {'bootstrap': False, 'max_depth': 110, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.732966 (0.020721) with: {'bootstrap': False, 'max_depth': 110, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.687363 (0.023948) with: {'bootstrap': False, 'max_depth': None, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.734708 (0.018844) with: {'bootstrap': False, 'max_depth': None, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.735597 (0.022715) with: {'bootstrap': False, 'max_depth': None, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.735597 (0.022715) with: {'bootstrap': False, 'max_depth': None, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.687363 (0.023948) with: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.734708 (0.018844) with: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.735597 (0.022715) with: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.735597 (0.022715) with: {'bootstrap': False, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5}\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state = 0, n_estimators = 200)\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 5)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5] #10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2] #4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "grid = {\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X_val,y_val)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7189e6",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da313249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.752222 using {'C': 1.0, 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
      "0.742599 (0.040004) with: {'C': 0.7, 'loss': 'hinge', 'penalty': 'l2'}\n",
      "0.750471 (0.029141) with: {'C': 0.7, 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
      "0.744350 (0.039736) with: {'C': 0.71578947, 'loss': 'hinge', 'penalty': 'l2'}\n",
      "0.750471 (0.029141) with: {'C': 0.71578947, 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
      "0.745223 (0.036826) with: {'C': 0.73157895, 'loss': 'hinge', 'penalty': 'l2'}\n",
      "0.749598 (0.029044) with: {'C': 0.73157895, 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
      "0.745219 (0.034377) with: {'C': 0.74736842, 'loss': 'hinge', 'penalty': 'l2'}\n",
      "0.748721 (0.029348) with: {'C': 0.74736842, 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
      "0.745223 (0.031432) with: {'C': 0.76315789, 'loss': 'hinge', 'penalty': 'l2'}\n",
      "0.748717 (0.029510) with: {'C': 0.76315789, 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
      "0.746966 (0.031743) with: {'C': 0.77894737, 'loss': 'hinge', 'penalty': 'l2'}\n",
      "0.746966 (0.027871) with: {'C': 0.77894737, 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
      "0.746970 (0.030966) with: {'C': 0.79473684, 'loss': 'hinge', 'penalty': 'l2'}\n",
      "0.748713 (0.025512) with: {'C': 0.79473684, 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
      "0.748724 (0.032062) with: {'C': 0.81052632, 'loss': 'hinge', 'penalty': 'l2'}\n",
      "0.749586 (0.024434) with: {'C': 0.81052632, 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
      "0.746093 (0.029136) with: {'C': 0.82631579, 'loss': 'hinge', 'penalty': 'l2'}\n",
      "0.751337 (0.024440) with: {'C': 0.82631579, 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
      "0.746974 (0.030559) with: {'C': 0.84210526, 'loss': 'hinge', 'penalty': 'l2'}\n",
      "0.750463 (0.024355) with: {'C': 0.84210526, 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
      "0.746101 (0.030616) with: {'C': 0.85789474, 'loss': 'hinge', 'penalty': 'l2'}\n",
      "0.750463 (0.024355) with: {'C': 0.85789474, 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
      "0.743469 (0.027981) with: {'C': 0.87368421, 'loss': 'hinge', 'penalty': 'l2'}\n",
      "0.750463 (0.024355) with: {'C': 0.87368421, 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
      "0.745216 (0.025573) with: {'C': 0.88947368, 'loss': 'hinge', 'penalty': 'l2'}\n",
      "0.750463 (0.024355) with: {'C': 0.88947368, 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
      "0.745219 (0.028243) with: {'C': 0.90526316, 'loss': 'hinge', 'penalty': 'l2'}\n",
      "0.750463 (0.024355) with: {'C': 0.90526316, 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
      "0.745219 (0.027831) with: {'C': 0.92105263, 'loss': 'hinge', 'penalty': 'l2'}\n",
      "0.750467 (0.025997) with: {'C': 0.92105263, 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
      "0.746974 (0.029152) with: {'C': 0.93684211, 'loss': 'hinge', 'penalty': 'l2'}\n",
      "0.749594 (0.026181) with: {'C': 0.93684211, 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
      "0.746974 (0.027523) with: {'C': 0.95263158, 'loss': 'hinge', 'penalty': 'l2'}\n",
      "0.748717 (0.024559) with: {'C': 0.95263158, 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
      "0.745216 (0.027036) with: {'C': 0.96842105, 'loss': 'hinge', 'penalty': 'l2'}\n",
      "0.750467 (0.023996) with: {'C': 0.96842105, 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
      "0.746089 (0.025992) with: {'C': 0.98421053, 'loss': 'hinge', 'penalty': 'l2'}\n",
      "0.750467 (0.023996) with: {'C': 0.98421053, 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
      "0.745216 (0.028142) with: {'C': 1.0, 'loss': 'hinge', 'penalty': 'l2'}\n",
      "0.752222 (0.023736) with: {'C': 1.0, 'loss': 'squared_hinge', 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC(random_state = 0)\n",
    "penalty = ['l2']\n",
    "loss = ['hinge', 'squared_hinge']\n",
    "c_values = [0.7       , 0.71578947, 0.73157895, 0.74736842, 0.76315789,\n",
    "       0.77894737, 0.79473684, 0.81052632, 0.82631579, 0.84210526,\n",
    "       0.85789474, 0.87368421, 0.88947368, 0.90526316, 0.92105263,\n",
    "       0.93684211, 0.95263158, 0.96842105, 0.98421053, 1.        ]\n",
    "#define grid search\n",
    "grid = dict(penalty=penalty,loss=loss, C=c_values)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X_val,y_val)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c03e86",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687604c4",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b3decba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lg = LogisticRegression(random_state=0, C=1.17777778, penalty='l2', solver = 'liblinear', max_iter=1000) \n",
    "\n",
    "t0 = time.time()\n",
    "lg.fit(X_train,y_train)\n",
    "t1 = time.time() # ending time\n",
    "lg_train_time = t1-t0\n",
    "\n",
    "t0 = time.time()\n",
    "y_true, y_pred_lg = y_test, lg.predict(X_test)\n",
    "t1 = time.time() # ending time\n",
    "lg_pred_time = t1-t0\n",
    "\n",
    "lg_report = classification_report(y_true, y_pred_lg, output_dict=True)\n",
    "df_lg = pd.DataFrame(lg_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345ae3d7",
   "metadata": {},
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab861ca4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state = 0, bootstrap=True, max_depth=None, max_features='auto', min_samples_leaf=1, min_samples_split=5)\n",
    "\n",
    "t0 = time.time()\n",
    "rf.fit(X_train,y_train)\n",
    "t1 = time.time() # ending time\n",
    "rf_train_time = t1-t0\n",
    "\n",
    "t0 = time.time()\n",
    "y_true, y_pred_rf = y_test, rf.predict(X_test)\n",
    "t1 = time.time() # ending time\n",
    "rf_pred_time = t1-t0\n",
    "\n",
    "rf_report = classification_report(y_true, y_pred_rf, output_dict=True)\n",
    "df_rf = pd.DataFrame(rf_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314a2813",
   "metadata": {},
   "source": [
    "## LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "643a4f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_svc = LinearSVC(random_state = 0, C = 0.79473684, loss = 'hinge', penalty = 'l2')\n",
    "\n",
    "t0 = time.time()\n",
    "l_svc.fit(X_train,y_train)\n",
    "t1 = time.time() # ending time\n",
    "l_svc_train_time = t1-t0\n",
    "\n",
    "l_svc_score = l_svc.score(X_test,y_test)\n",
    "\n",
    "t0 = time.time()\n",
    "y_true, y_pred_lSVC = y_test, l_svc.predict(X_test)\n",
    "t1 = time.time() # ending time\n",
    "l_svc_pred_time = t1-t0\n",
    "\n",
    "l_svc_report = classification_report(y_true, y_pred_lSVC, output_dict=True)\n",
    "df_l_svc = pd.DataFrame(l_svc_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc5db25",
   "metadata": {},
   "source": [
    "## Table Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b08108a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_0e8f0_\">\n",
       "  <caption>ML Model Accuracy Results</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Logistic Regression</th>\n",
       "      <th class=\"col_heading level0 col1\" >RandomForestClassifier</th>\n",
       "      <th class=\"col_heading level0 col2\" >LinearSVC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0e8f0_level0_row0\" class=\"row_heading level0 row0\" >accuracy</th>\n",
       "      <td id=\"T_0e8f0_row0_col0\" class=\"data row0 col0\" >0.793345</td>\n",
       "      <td id=\"T_0e8f0_row0_col1\" class=\"data row0 col1\" >0.792469</td>\n",
       "      <td id=\"T_0e8f0_row0_col2\" class=\"data row0 col2\" >0.801226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f81c11f40d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_all = {'Logistic Regression': [df_lg.iloc[0]['accuracy']], \n",
    "                 'RandomForestClassifier': [df_rf.iloc[0]['accuracy']], \n",
    "                 'LinearSVC': [df_l_svc.iloc[0]['accuracy']]}\n",
    "acc_table = pd.DataFrame(data=acc_all)\n",
    "acc_table.index = ['accuracy']\n",
    "acc_table.style.set_caption(\"ML Model Accuracy Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82cbc128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_54f73_\">\n",
       "  <caption>Class 0 Results</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Logistic Regression</th>\n",
       "      <th class=\"col_heading level0 col1\" >RandomForestClassifier</th>\n",
       "      <th class=\"col_heading level0 col2\" >LinearSVC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_54f73_level0_row0\" class=\"row_heading level0 row0\" >precision</th>\n",
       "      <td id=\"T_54f73_row0_col0\" class=\"data row0 col0\" >0.774834</td>\n",
       "      <td id=\"T_54f73_row0_col1\" class=\"data row0 col1\" >0.791549</td>\n",
       "      <td id=\"T_54f73_row0_col2\" class=\"data row0 col2\" >0.782667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_54f73_level0_row1\" class=\"row_heading level0 row1\" >recall</th>\n",
       "      <td id=\"T_54f73_row1_col0\" class=\"data row1 col0\" >0.898618</td>\n",
       "      <td id=\"T_54f73_row1_col1\" class=\"data row1 col1\" >0.863287</td>\n",
       "      <td id=\"T_54f73_row1_col2\" class=\"data row1 col2\" >0.901690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_54f73_level0_row2\" class=\"row_heading level0 row2\" >f1-score</th>\n",
       "      <td id=\"T_54f73_row2_col0\" class=\"data row2 col0\" >0.832148</td>\n",
       "      <td id=\"T_54f73_row2_col1\" class=\"data row2 col1\" >0.825863</td>\n",
       "      <td id=\"T_54f73_row2_col2\" class=\"data row2 col2\" >0.837973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f81c4b34100>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#class 0 info\n",
    "c_0_all = {'Logistic Regression': [df_lg.iloc[0]['0'], df_lg.iloc[1]['0'], df_lg.iloc[2]['0']], \n",
    "                 'RandomForestClassifier': [df_rf.iloc[0]['0'], df_rf.iloc[1]['0'], df_rf.iloc[2]['0']], \n",
    "                 'LinearSVC': [df_l_svc.iloc[0]['0'], df_l_svc.iloc[1]['0'], df_l_svc.iloc[2]['0']]}\n",
    "c_0_table = pd.DataFrame(data=c_0_all)\n",
    "c_0_table.index = ['precision', 'recall', 'f1-score']\n",
    "c_0_table.style.set_caption(\"Class 0 Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fe59da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_020ac_\">\n",
       "  <caption>Class 1 Results</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Logistic Regression</th>\n",
       "      <th class=\"col_heading level0 col1\" >RandomForestClassifier</th>\n",
       "      <th class=\"col_heading level0 col2\" >LinearSVC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_020ac_level0_row0\" class=\"row_heading level0 row0\" >precision</th>\n",
       "      <td id=\"T_020ac_row0_col0\" class=\"data row0 col0\" >0.829457</td>\n",
       "      <td id=\"T_020ac_row0_col1\" class=\"data row0 col1\" >0.793981</td>\n",
       "      <td id=\"T_020ac_row0_col2\" class=\"data row0 col2\" >0.836735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_020ac_level0_row1\" class=\"row_heading level0 row1\" >recall</th>\n",
       "      <td id=\"T_020ac_row1_col0\" class=\"data row1 col0\" >0.653768</td>\n",
       "      <td id=\"T_020ac_row1_col1\" class=\"data row1 col1\" >0.698574</td>\n",
       "      <td id=\"T_020ac_row1_col2\" class=\"data row1 col2\" >0.668024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_020ac_level0_row2\" class=\"row_heading level0 row2\" >f1-score</th>\n",
       "      <td id=\"T_020ac_row2_col0\" class=\"data row2 col0\" >0.731207</td>\n",
       "      <td id=\"T_020ac_row2_col1\" class=\"data row2 col1\" >0.743229</td>\n",
       "      <td id=\"T_020ac_row2_col2\" class=\"data row2 col2\" >0.742922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f81a542c9d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#class 1 info\n",
    "c_1_all = {'Logistic Regression': [df_lg.iloc[0]['1'], df_lg.iloc[1]['1'], df_lg.iloc[2]['1']], \n",
    "                 'RandomForestClassifier': [df_rf.iloc[0]['1'], df_rf.iloc[1]['1'], df_rf.iloc[2]['1']], \n",
    "                 'LinearSVC': [df_l_svc.iloc[0]['1'], df_l_svc.iloc[1]['1'], df_l_svc.iloc[2]['1']]}\n",
    "c_1_table = pd.DataFrame(data=c_1_all)\n",
    "c_1_table.index = ['precision', 'recall', 'f1-score']\n",
    "c_1_table.style.set_caption(\"Class 1 Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89b7394c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_6004e_\">\n",
       "  <caption>ML Train and Prediction Times</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Logistic Regression</th>\n",
       "      <th class=\"col_heading level0 col1\" >RandomForestClassifier</th>\n",
       "      <th class=\"col_heading level0 col2\" >LinearSVC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6004e_level0_row0\" class=\"row_heading level0 row0\" >train time</th>\n",
       "      <td id=\"T_6004e_row0_col0\" class=\"data row0 col0\" >0.010318</td>\n",
       "      <td id=\"T_6004e_row0_col1\" class=\"data row0 col1\" >3.170267</td>\n",
       "      <td id=\"T_6004e_row0_col2\" class=\"data row0 col2\" >0.030324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6004e_level0_row1\" class=\"row_heading level0 row1\" >prediction time</th>\n",
       "      <td id=\"T_6004e_row1_col0\" class=\"data row1 col0\" >0.000343</td>\n",
       "      <td id=\"T_6004e_row1_col1\" class=\"data row1 col1\" >0.132451</td>\n",
       "      <td id=\"T_6004e_row1_col2\" class=\"data row1 col2\" >0.000175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f81a542c5e0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_all = {'Logistic Regression': [lg_train_time, lg_pred_time], \n",
    "                 'RandomForestClassifier': [rf_train_time, rf_pred_time], \n",
    "                 'LinearSVC': [l_svc_train_time, l_svc_pred_time]}\n",
    "time_table = pd.DataFrame(data=time_all)\n",
    "time_table.index = ['train time', 'prediction time']\n",
    "time_table.style.set_caption(\"ML Train and Prediction Times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5066ec53",
   "metadata": {},
   "source": [
    "# DISCUSSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7840e0b2",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f30e04",
   "metadata": {},
   "source": [
    "Of the three machine learning algorithms implemented, LinearSVC resulted in the highest overall classification accuracy at 0.801226. Both Logistic Regression and RandomForestClassifier were right behind in terms of accuracy though. Regarding the Class 0 results, again LinearSVC performed the best and had the highest recall and f-1 score, however, RandomForestClassifier resulted in the highest precision. The results were a bit different for the class 1 results. RandomForestClassifier outperformed the other two models in recall and f-1 score but LinearSVC had the highest precision. Directly above you can see the train and prediction times. RandomForestClassifier took by far the longest to train and predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc5a6cc",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38d04af",
   "metadata": {},
   "source": [
    "### What kinds of tweets/language are consistently misclassified?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcb8dc9",
   "metadata": {},
   "source": [
    "I decided that I wanted to examine if there were any tweets that were misclassified by ALL the models implemented. Interestingly, there were 167 tweets from the test set that none of the models correctly classified. This is a remarkably large number of the test set at nearly 15% of the tweets. I dove in and removed all of the stopwords from these tweets and used a counter to examine which words appeared with the most frequency within the tweets. A few notable words were: ('snowstorm', 6) ('refugees', 5), ('nuclear', 5), ('fire', 5), ('trapped', 5), ('burning', 5), ('mass', 5), ('weapons', 4), ('emergency', 4), ('bioterror', 4), ('dead', 3), ('electrocuted', 3), and ('death', 3). In some cases, the tweets are misclassified when disaster terms are used to describe other things. For example: tweet 6086 – “years afloat pension plans start sinking”.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ed5c704",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#find which tweets were misclasified by each model\n",
    "misclas_lg = (y_pred_lg != y_true)\n",
    "misclas_rf = (y_pred_rf != y_true)\n",
    "misclas_svc = (y_pred_lSVC != y_true)\n",
    "\n",
    "#pull only the tweet that were misclasified\n",
    "misclas_lg = misclas_lg[misclas_lg == True]\n",
    "misclas_rf = misclas_rf[misclas_rf == True]\n",
    "misclas_svc = misclas_svc[misclas_svc == True]\n",
    "\n",
    "#convert tweet idx to a list\n",
    "misclas_lg_idk = list(misclas_lg.index.values)\n",
    "misclas_rf_idk = list(misclas_rf.index.values)\n",
    "misclas_svc_idk = list(misclas_svc.index.values)\n",
    "\n",
    "#find matches between the 3 models\n",
    "all_misclas = set(misclas_rf_idk) & set(misclas_svc_idk) & set(misclas_lg_idk) \n",
    "\n",
    "#all of the missclas and lowercase\n",
    "dF_tweets_misclas = dC_x_fin[all_misclas]\n",
    "dF_tweets_misclas = dF_tweets_misclas.str.lower()\n",
    "\n",
    "#remove stopwords\n",
    "stop = stopwords.words('english')\n",
    "what_words = dF_tweets_misclas.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "#words that were in tweets that were misclassified by all models\n",
    "miss_count = Counter(\" \".join(what_words).split()).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1050583a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5633    one couple using drones save refugees world de...\n",
       "4609    prediction vikings game sunday dont expect who...\n",
       "2564    6 destroy reg c competitiveness entire region ...\n",
       "7175                       one direction concert war zone\n",
       "5128    government pressure abandon plans construct uk...\n",
       "2569                  sj gist houses farm produce destroy\n",
       "5642    newlyweds feed thousands syrian refugees inste...\n",
       "6667    purple heart vet finds jihad threat car mall i...\n",
       "5648    chpsre rt refugees followers paris visit dream...\n",
       "4115                                           gotta love\n",
       "5142                little filming inside nuclear reactor\n",
       "535                               chiasson sens come deal\n",
       "6167                    thu aug 20 32 gmt 0000 utc sirens\n",
       "4121                               ready close errrr nope\n",
       "2079                              said dead one else dies\n",
       "544                                 avalanche city sunset\n",
       "38      barbados jamaica two cars set ablaze santa cru...\n",
       "3117    blow drying hair amp cable caught fire let go ...\n",
       "4143    love picked playing worth fifth harm kid ink l...\n",
       "6703    l b durant nba adidas oklahoma city thunder yo...\n",
       "7218    agreed especially automatic weapons legitimate...\n",
       "3123    zotar 50 skeleton alchemist electrocuted death...\n",
       "7221    incredulous continued outcry welfare waste tax...\n",
       "3638                    osp concerned mounting fatalities\n",
       "3131             student electrocuted death school campus\n",
       "61      progressive greetings month students would set...\n",
       "3134                               emergency flow mp3 rar\n",
       "577     jacksonville busines fedex stops shipping pote...\n",
       "6212    know smoke way make taxis buses come light cig...\n",
       "7238                        forgotten hope secret weapons\n",
       "3656    policylab last public hearing nyc today tomorr...\n",
       "1614                       correction tent collapse story\n",
       "597            fedex longer transport bioterror germs via\n",
       "6230    manuel hoping early buffalo snowstorm accuracy...\n",
       "598     jax biz journal fedex stops shipping potential...\n",
       "1625    growth dries bhp billiton oil price collapse b...\n",
       "602     fedex stops shipping potential bioterror patho...\n",
       "2143    real magic real life women went missing ohio t...\n",
       "6239    bring back games past snowstorm tic tac toe ba...\n",
       "1121                 redskins wr roberts belly bombed via\n",
       "610                                fight bioterrorism sir\n",
       "1632    way gary chicago entrances closed due bridge c...\n",
       "5732            rescuers found barely alive idea allergic\n",
       "6241                        still got snowstorm hailstorm\n",
       "6760                                photo day stormchaser\n",
       "1645    prices insane dollar collapsed us punishing us...\n",
       "6253    photo mothernaturenetwork thundersnow hearing ...\n",
       "6256    actually currently dressed snowstorm despite m...\n",
       "2162    pls reduce cyclist deaths compulsory highway c...\n",
       "3186    b c costs less sick people using emergency roo...\n",
       "2675                           apollo brown detonate ft p\n",
       "5751                              selfie booth riot kappa\n",
       "6278                                        storm silence\n",
       "2704    detonation fashionable mountaineering electron...\n",
       "3730               walking dead spin fear walking dead rd\n",
       "5267                                    slicker oil spill\n",
       "4249    middle humid heat wave patch forehead flared t...\n",
       "3228    goulburn man henry van bilsen missing emergenc...\n",
       "2718        amp devastated amp 1000 indianperpetrated amp\n",
       "2207               towboat trek sympathy deluge falls vtc\n",
       "6818    bomb head explosive decisions dat produced dea...\n",
       "163                                           believe bro\n",
       "4772                              crazy lightning outside\n",
       "3755                             fire way close wtf going\n",
       "1707    scored points punch quest stopped squeaky bat ...\n",
       "5805    people rioting everywhere think one usami san ...\n",
       "2222    perhaps historic applied deluge recently expos...\n",
       "6837    hollywood movie trapped miners released chile ...\n",
       "3256    manifesting one right man helm save sinking sh...\n",
       "6840    hollywood movie trapped miners released chile ...\n",
       "3770                                         checked fire\n",
       "3768                                  wanna set shit fire\n",
       "2239    uk deluge canadian themed tops around timing p...\n",
       "4807    st steel coffee cafetiere exploded loud bang h...\n",
       "3272    lucas duda ghost rider nic cage version actual...\n",
       "5321    pandemonium aba woman delivers baby without fa...\n",
       "1224    one challenges tough enough rescuing people bu...\n",
       "719                        stab back promise one bleeding\n",
       "2773    fascinating pics inside north korea propaganda...\n",
       "4318    prophet peace upon said save hellfire even giv...\n",
       "4831    made good point white person comings mass murd...\n",
       "4835    media needs stop publicizing mass murder many ...\n",
       "7401    whole food stamp gov assistance program needs ...\n",
       "2285        abbswinston demolish structures jordan valley\n",
       "4845                                hillary mass murderer\n",
       "243     hell fraction belief total annihilation destru...\n",
       "1279          burned dog finds new home young burn victim\n",
       "4863             step one get mass murderer portrait yuan\n",
       "4356    gov brown allows parole chowchilla school bus ...\n",
       "1798                                                crash\n",
       "264          stop annihilation salt river wild horses via\n",
       "5902                                        put sandstorm\n",
       "2832    potus strategy refugees idp internally displac...\n",
       "2834                        circular ruins displaced part\n",
       "4376    governor weighs parole california school bus h...\n",
       "7453    thought surgical wounds healed weather helping...\n",
       "1823    heard days ago driving near crashed car laughi...\n",
       "4392    funtenna hijacking computers send data sound w...\n",
       "3383    school put evacuation alarms accidently differ...\n",
       "5435    maid charged stealing dh30000 police officer s...\n",
       "1853                                 ina buted girl crush\n",
       "1348    last retweet would think lion saved people bur...\n",
       "1861               ron amp fez dave high school crush via\n",
       "1350    battling monsters pulling burning buildings sa...\n",
       "842                              call little bit blizzard\n",
       "1358    firefighters acted like cops drive around shoo...\n",
       "848                                             bruh dies\n",
       "3411    kendall jenner nick jonas dating world might q...\n",
       "5463    south gate police officers huntington park off...\n",
       "5467    updates promises quarantine extremely offensiv...\n",
       "6494                      fuck sake john jesus heart sunk\n",
       "1891                                man crushed death car\n",
       "356      build army dogs leader lion dogs fight like lion\n",
       "361     victorinox swiss army date women rubber mop watch\n",
       "3435          chick masturbates guy gets exploded face gt\n",
       "4973    byproduct metal price meltdown higher silver p...\n",
       "2416                  derailed steps cleaning train wreck\n",
       "3441             dress memes officially exploded internet\n",
       "6006    agree certain cultural appropriation things ho...\n",
       "894     1 russia may played reason link bs okanowa blo...\n",
       "7552                       israel wrecked home wants land\n",
       "2944    jacksonville family bands together memorial pl...\n",
       "5507    reddit new content policy goes effect many hor...\n",
       "4488    per previous behaviour jan aq would deal kidna...\n",
       "2961                 london drowning iiii live riveeeeeer\n",
       "2962    family mourns drowning superhero toddler rare ...\n",
       "5017    year old smug face dorret brings mudslide blac...\n",
       "6042    england east coast dogger bank westward seismi...\n",
       "2970             like theres fire skin im drowning within\n",
       "7071    long lord study sixth seal opens events revela...\n",
       "1440    unpredictable disconnected social casualty fav...\n",
       "929                  confederate ship blown crew read via\n",
       "2464    mumbai24x7 helping hand mumbai ttes take charg...\n",
       "4004    hope rains throughout whole weekend hope flood...\n",
       "5031               mudslide like chewing rubber tyre done\n",
       "6569    dear name humanityi apologized survivors r u r...\n",
       "5035                                             mudslide\n",
       "4524                                                 prob\n",
       "5038      oso washington mudslide response interview part\n",
       "4536                likely killed injured killer gun isis\n",
       "6584    survived magic searching wood survivors outsid...\n",
       "6585    feel pain survivors look back period absurd hu...\n",
       "2491     unexercised honda run neighborhood desolate psqd\n",
       "3003    dust devil maintenance fee buy la rotary storm...\n",
       "4029    liked video j cole fire squad 2014 forest hill...\n",
       "2493    ty follow go brutally abused desolate amp lost...\n",
       "6086             years afloat pension plans start sinking\n",
       "2503    abomination maketh desolate antichrist desecra...\n",
       "7111    causing entire sky around battle darken violen...\n",
       "3021    wall noise one thing wall dust moving mph get ...\n",
       "4569    baby girls car wreak afternoon thank god serio...\n",
       "473     strongly condemn attack ary news team karachi ...\n",
       "6619    really cannot condemn entire group based actio...\n",
       "6108           feel like sinking low self image take quiz\n",
       "6623                                                truth\n",
       "991                                      body bagging mfs\n",
       "6631    signing petition seek mercy death punishment c...\n",
       "1512    scientists dont believe catastrophic man made ...\n",
       "4585    carterville high school coaches prepare game d...\n",
       "5613    married turkish couple gave syrian refugees in...\n",
       "494                telnet attacked streamyx home southern\n",
       "3057    km nne geysers california time2015 08 05 40 21...\n",
       "7157    thank neighborhood looks like war zone power b...\n",
       "5114    chernobyl disaster wikipedia free encyclopedia...\n",
       "4095                                           comes hail\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#misclassified tweets\n",
    "pd.set_option('display.max_rows', None)\n",
    "what_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25a45283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 10),\n",
       " ('amp', 9),\n",
       " ('people', 8),\n",
       " ('like', 8),\n",
       " ('via', 6),\n",
       " ('fire', 6),\n",
       " ('school', 6),\n",
       " ('refugees', 5),\n",
       " ('dead', 5),\n",
       " ('snowstorm', 5),\n",
       " ('man', 5),\n",
       " ('trapped', 5),\n",
       " ('buildings', 5),\n",
       " ('nuclear', 4),\n",
       " ('car', 4),\n",
       " ('death', 4),\n",
       " ('fedex', 4),\n",
       " ('bioterror', 4),\n",
       " ('hope', 4),\n",
       " ('back', 4),\n",
       " ('hollywood', 4),\n",
       " ('movie', 4),\n",
       " ('miners', 4),\n",
       " ('burning', 4),\n",
       " ('mass', 4),\n",
       " ('mudslide', 4),\n",
       " ('save', 3),\n",
       " ('world', 3),\n",
       " ('game', 3),\n",
       " ('whole', 3),\n",
       " ('think', 3),\n",
       " ('st', 3),\n",
       " ('c', 3),\n",
       " ('entire', 3),\n",
       " ('b', 3),\n",
       " ('heart', 3),\n",
       " ('love', 3),\n",
       " ('come', 3),\n",
       " ('dies', 3),\n",
       " ('set', 3),\n",
       " ('police', 3),\n",
       " ('get', 3),\n",
       " ('electrocuted', 3),\n",
       " ('weapons', 3),\n",
       " ('would', 3),\n",
       " ('emergency', 3),\n",
       " ('stops', 3),\n",
       " ('shipping', 3),\n",
       " ('potential', 3),\n",
       " ('pathogens', 3),\n",
       " ('way', 3),\n",
       " ('oil', 3),\n",
       " ('price', 3),\n",
       " ('us', 3),\n",
       " ('day', 3),\n",
       " ('storm', 3),\n",
       " ('take', 3),\n",
       " ('deluge', 3),\n",
       " ('believe', 3),\n",
       " ('sinking', 3),\n",
       " ('done', 3),\n",
       " ('around', 3),\n",
       " ('exploded', 3),\n",
       " ('face', 3),\n",
       " ('murder', 3),\n",
       " ('home', 3),\n",
       " ('lion', 3),\n",
       " ('family', 3),\n",
       " ('drowning', 3),\n",
       " ('survivors', 3),\n",
       " ('desolate', 3),\n",
       " ('couple', 2),\n",
       " ('using', 2),\n",
       " ('dont', 2),\n",
       " ('free', 2),\n",
       " ('destroy', 2),\n",
       " ('gets', 2),\n",
       " ('war', 2),\n",
       " ('zone', 2),\n",
       " ('plans', 2),\n",
       " ('reactor', 2),\n",
       " ('year', 2),\n",
       " ('syrian', 2),\n",
       " ('instead', 2),\n",
       " ('finds', 2),\n",
       " ('_', 2),\n",
       " ('little', 2),\n",
       " ('inside', 2),\n",
       " ('deal', 2),\n",
       " ('ready', 2),\n",
       " ('close', 2),\n",
       " ('said', 2),\n",
       " ('city', 2),\n",
       " ('ablaze', 2),\n",
       " ('head', 2),\n",
       " ('cable', 2),\n",
       " ('let', 2),\n",
       " ('go', 2),\n",
       " ('thunder', 2),\n",
       " ('reason', 2)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count of words that are in the above tweets\n",
    "miss_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b621594",
   "metadata": {},
   "source": [
    "### Does TFIDF do better than plain old bag of words?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31cb2f7",
   "metadata": {},
   "source": [
    "For the below questions, I am going to use the ML model with the hyperparameters that performed to obtain answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a15161",
   "metadata": {},
   "source": [
    "It is evident that the use of TFIDF results in a higher accuracy than when the plain old bag of words is implemented. TFIDF had an accuracy of 4.4809 percent higher than the plain old bag of words. As I mentioned at the beginning this may have something to do with how the CountVectorizer can introduce a bias for the most frequent words that appear in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac481753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process raw text into ML compatible features\n",
    "vectorizer = TfidfVectorizer(min_df=3, \n",
    "             stop_words='english',ngram_range=(1, 2), lowercase=True)  \n",
    "vectorizer.fit(dC_x_fin)\n",
    "#print(vectorizer.vocabulary_)\n",
    "X = vectorizer.transform(dC_x_fin)\n",
    "#vectorizer.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64220542",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, dC_y, \n",
    "                                   test_size=0.15, shuffle=True, stratify=dC_y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                 test_size=0.15/0.85, shuffle=True, stratify=y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "626d30cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With TFIDF, Logistic regression gives an accuracy of: 0.8012259194395797\n"
     ]
    }
   ],
   "source": [
    "l_svc = LinearSVC(random_state = 0, C = 0.79473684, loss = 'hinge', penalty = 'l2')\n",
    "\n",
    "l_svc.fit(X_train,y_train)\n",
    "\n",
    "l_svc_score = l_svc.score(X_test,y_test)\n",
    "\n",
    "print(f'With TFIDF, Logistic regression gives an accuracy of: {l_svc_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aee12e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectorizer = CountVectorizer(min_df=3, \n",
    "             stop_words='english',ngram_range=(1, 2), lowercase=True) \n",
    "XX = countVectorizer.fit_transform(dC_x_fin)\n",
    "#countVectorizer.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0a0e430",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(XX, dC_y, \n",
    "                                   test_size=0.15, shuffle=True, stratify=dC_y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                 test_size=0.15/0.85, shuffle=True, stratify=y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4838d12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With CountVectorizer, Logistic regression gives an accuracy of: 0.7653239929947461\n"
     ]
    }
   ],
   "source": [
    "l_svc = LinearSVC(random_state = 0, C = 0.79473684, loss = 'hinge', penalty = 'l2') \n",
    "\n",
    "l_svc.fit(X_train,y_train)\n",
    "\n",
    "l_svc_score = l_svc.score(X_test,y_test)\n",
    "\n",
    "print(f'With CountVectorizer, Logistic regression gives an accuracy of: {l_svc_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e86e982",
   "metadata": {},
   "source": [
    "### What's better, lowercasing or not lowercasing text?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb0cdac",
   "metadata": {},
   "source": [
    "To my surprise, for the experiment run above not lowercasing the text resulted in a slightly higher classification accuracy. The difference though is quite small. Non-lowercased data resulted in a 0.2181 percent higher classification accuracy. It seems that my inference and initial tests regarding whether or not the text should be lower case may be false. I am not sure why or if it really matters since it is so low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8ebebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process raw text into ML compatible features\n",
    "vectorizer = TfidfVectorizer(min_df=3, \n",
    "             stop_words='english',ngram_range=(1, 2), lowercase=True)  \n",
    "vectorizer.fit(dC_x_fin)\n",
    "#print(vectorizer.vocabulary_)\n",
    "X = vectorizer.transform(dC_x_fin)\n",
    "#vectorizer.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6215369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, dC_y, \n",
    "                                   test_size=0.15, shuffle=True, stratify=dC_y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                 test_size=0.15/0.85, shuffle=True, stratify=y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82299d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercase=True, Logistic regression gives an accuracy of: 0.8012259194395797\n"
     ]
    }
   ],
   "source": [
    "l_svc = LinearSVC(random_state = 0, C = 0.79473684, loss = 'hinge', penalty = 'l2') \n",
    "\n",
    "l_svc.fit(X_train,y_train)\n",
    "\n",
    "l_svc_score = l_svc.score(X_test,y_test)\n",
    "\n",
    "print(f'Lowercase=True, Logistic regression gives an accuracy of: {l_svc_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e35eedd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process raw text into ML compatible features\n",
    "vectorizer = TfidfVectorizer(min_df=3, \n",
    "             stop_words='english',ngram_range=(1, 2), lowercase=False)  \n",
    "vectorizer.fit(dC_x_fin)\n",
    "#print(vectorizer.vocabulary_)\n",
    "X = vectorizer.transform(dC_x_fin)\n",
    "#vectorizer.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a30575a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, dC_y, \n",
    "                                   test_size=0.15, shuffle=True, stratify=dC_y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "                                 test_size=0.15/0.85, shuffle=True, stratify=y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1068e82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercase=False, Logistic regression gives an accuracy of: 0.8029772329246935\n"
     ]
    }
   ],
   "source": [
    "l_svc = LinearSVC(random_state = 0, C = 0.79473684, loss = 'hinge', penalty = 'l2')  \n",
    "\n",
    "l_svc.fit(X_train,y_train)\n",
    "\n",
    "l_svc_score = l_svc.score(X_test,y_test)\n",
    "\n",
    "print(f'Lowercase=False, Logistic regression gives an accuracy of: {l_svc_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b1a13f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
